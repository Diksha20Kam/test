{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d1c40fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "String=\"This is something. this\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eb1143c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is something. this'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "String.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e05f6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERTcase:\n",
    "    BERT uncased : text is lowercased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d7f68f",
   "metadata": {},
   "source": [
    "### Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b49b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "the\n",
    "and\n",
    "a\n",
    "an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16fef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "har documment me /corpus me iski frequency jada\n",
    "rahti hai isliye accuracy increase krne k liye \n",
    "stopwords remove krte hai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bf592d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/admin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b547e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "debc6352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopword=stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3598f3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "036de091",
   "metadata": {},
   "outputs": [],
   "source": [
    "string=\"The sun.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b259144e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sun.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join([word for word in string.lower().split() if word not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31951588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'basque', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28544c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "hind_stopwords=stopwords.words(\"hinglish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb0be2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'aadi', 'aaj', 'aap', 'aapne', 'aata', 'aati', 'aaya', 'aaye', 'ab', 'abbe', 'abbey', 'abe', 'abhi', 'able', 'about', 'above', 'accha', 'according', 'accordingly', 'acha', 'achcha', 'across', 'actually', 'after', 'afterwards', 'again', 'against', 'agar', 'ain', 'aint', \"ain't\", 'aisa', 'aise', 'aisi', 'alag', 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'andar', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'ap', 'apan', 'apart', 'apna', 'apnaa', 'apne', 'apni', 'appear', 'are', 'aren', 'arent', \"aren't\", 'around', 'arre', 'as', 'aside', 'ask', 'asking', 'at', 'aur', 'avum', 'aya', 'aye', 'baad', 'baar', 'bad', 'bahut', 'bana', 'banae', 'banai', 'banao', 'banaya', 'banaye', 'banayi', 'banda', 'bande', 'bandi', 'bane', 'bani', 'bas', 'bata', 'batao', 'bc', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'bhai', 'bheetar', 'bhi', 'bhitar', 'bht', 'bilkul', 'bohot', 'bol', 'bola', 'bole', 'boli', 'bolo', 'bolta', 'bolte', 'bolti', 'both', 'brief', 'bro', 'btw', 'but', 'by', 'came', 'can', 'cannot', 'cant', \"can't\", 'cause', 'causes', 'certain', 'certainly', 'chahiye', 'chaiye', 'chal', 'chalega', 'chhaiye', 'clearly', \"c'mon\", 'com', 'come', 'comes', 'could', 'couldn', 'couldnt', \"couldn't\", 'd', 'de', 'dede', 'dega', 'degi', 'dekh', 'dekha', 'dekhe', 'dekhi', 'dekho', 'denge', 'dhang', 'di', 'did', 'didn', 'didnt', \"didn't\", 'dijiye', 'diya', 'diyaa', 'diye', 'diyo', 'do', 'does', 'doesn', 'doesnt', \"doesn't\", 'doing', 'done', 'dono', 'dont', \"don't\", 'doosra', 'doosre', 'down', 'downwards', 'dude', 'dunga', 'dungi', 'during', 'dusra', 'dusre', 'dusri', 'dvaara', 'dvara', 'dwaara', 'dwara', 'each', 'edu', 'eg', 'eight', 'either', 'ek', 'else', 'elsewhere', 'enough', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'far', 'few', 'fifth', 'fir', 'first', 'five', 'followed', 'following', 'follows', 'for', 'forth', 'four', 'from', 'further', 'furthermore', 'gaya', 'gaye', 'gayi', 'get', 'gets', 'getting', 'ghar', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'good', 'got', 'gotten', 'greetings', 'haan', 'had', 'hadd', 'hadn', 'hadnt', \"hadn't\", 'hai', 'hain', 'hamara', 'hamare', 'hamari', 'hamne', 'han', 'happens', 'har', 'hardly', 'has', 'hasn', 'hasnt', \"hasn't\", 'have', 'haven', 'havent', \"haven't\", 'having', 'he', 'hello', 'help', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', \"here's\", 'hereupon', 'hers', 'herself', \"he's\", 'hi', 'him', 'himself', 'his', 'hither', 'hm', 'hmm', 'ho', 'hoga', 'hoge', 'hogi', 'hona', 'honaa', 'hone', 'honge', 'hongi', 'honi', 'hopefully', 'hota', 'hotaa', 'hote', 'hoti', 'how', 'howbeit', 'however', 'hoyenge', 'hoyengi', 'hu', 'hua', 'hue', 'huh', 'hui', 'hum', 'humein', 'humne', 'hun', 'huye', 'huyi', 'i', \"i'd\", 'idk', 'ie', 'if', \"i'll\", \"i'm\", 'imo', 'in', 'inasmuch', 'inc', 'inhe', 'inhi', 'inho', 'inka', 'inkaa', 'inke', 'inki', 'inn', 'inner', 'inse', 'insofar', 'into', 'inward', 'is', 'ise', 'isi', 'iska', 'iskaa', 'iske', 'iski', 'isme', 'isn', 'isne', 'isnt', \"isn't\", 'iss', 'isse', 'issi', 'isski', 'it', \"it'd\", \"it'll\", 'itna', 'itne', 'itni', 'itno', 'its', \"it's\", 'itself', 'ityaadi', 'ityadi', \"i've\", 'ja', 'jaa', 'jab', 'jabh', 'jaha', 'jahaan', 'jahan', 'jaisa', 'jaise', 'jaisi', 'jata', 'jayega', 'jidhar', 'jin', 'jinhe', 'jinhi', 'jinho', 'jinhone', 'jinka', 'jinke', 'jinki', 'jinn', 'jis', 'jise', 'jiska', 'jiske', 'jiski', 'jisme', 'jiss', 'jisse', 'jitna', 'jitne', 'jitni', 'jo', 'just', 'jyaada', 'jyada', 'k', 'ka', 'kaafi', 'kab', 'kabhi', 'kafi', 'kaha', 'kahaa', 'kahaan', 'kahan', 'kahi', 'kahin', 'kahte', 'kaisa', 'kaise', 'kaisi', 'kal', 'kam', 'kar', 'kara', 'kare', 'karega', 'karegi', 'karen', 'karenge', 'kari', 'karke', 'karna', 'karne', 'karni', 'karo', 'karta', 'karte', 'karti', 'karu', 'karun', 'karunga', 'karungi', 'kaun', 'kaunsa', 'kayi', 'kch', 'ke', 'keep', 'keeps', 'keh', 'kehte', 'kept', 'khud', 'ki', 'kin', 'kine', 'kinhe', 'kinho', 'kinka', 'kinke', 'kinki', 'kinko', 'kinn', 'kino', 'kis', 'kise', 'kisi', 'kiska', 'kiske', 'kiski', 'kisko', 'kisliye', 'kisne', 'kitna', 'kitne', 'kitni', 'kitno', 'kiya', 'kiye', 'know', 'known', 'knows', 'ko', 'koi', 'kon', 'konsa', 'koyi', 'krna', 'krne', 'kuch', 'kuchch', 'kuchh', 'kul', 'kull', 'kya', 'kyaa', 'kyu', 'kyuki', 'kyun', 'kyunki', 'lagta', 'lagte', 'lagti', 'last', 'lately', 'later', 'le', 'least', 'lekar', 'lekin', 'less', 'lest', 'let', \"let's\", 'li', 'like', 'liked', 'likely', 'little', 'liya', 'liye', 'll', 'lo', 'log', 'logon', 'lol', 'look', 'looking', 'looks', 'ltd', 'lunga', 'm', 'maan', 'maana', 'maane', 'maani', 'maano', 'magar', 'mai', 'main', 'maine', 'mainly', 'mana', 'mane', 'mani', 'mano', 'many', 'mat', 'may', 'maybe', 'me', 'mean', 'meanwhile', 'mein', 'mera', 'mere', 'merely', 'meri', 'might', 'mightn', 'mightnt', \"mightn't\", 'mil', 'mjhe', 'more', 'moreover', 'most', 'mostly', 'much', 'mujhe', 'must', 'mustn', 'mustnt', \"mustn't\", 'my', 'myself', 'na', 'naa', 'naah', 'nahi', 'nahin', 'nai', 'name', 'namely', 'nd', 'ne', 'near', 'nearly', 'necessary', 'neeche', 'need', 'needn', 'neednt', \"needn't\", 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nhi', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nope', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'par', 'pata', 'pe', 'pehla', 'pehle', 'pehli', 'people', 'per', 'perhaps', 'phla', 'phle', 'phli', 'placed', 'please', 'plus', 'poora', 'poori', 'provides', 'pura', 'puri', 'q', 'que', 'quite', 'raha', 'rahaa', 'rahe', 'rahi', 'rakh', 'rakha', 'rakhe', 'rakhen', 'rakhi', 'rakho', 'rather', 're', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'rehte', 'rha', 'rhaa', 'rhe', 'rhi', 'ri', 'right', 's', 'sa', 'saara', 'saare', 'saath', 'sab', 'sabhi', 'sabse', 'sahi', 'said', 'sakta', 'saktaa', 'sakte', 'sakti', 'same', 'sang', 'sara', 'sath', 'saw', 'say', 'saying', 'says', 'se', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'shan', 'shant', \"shan't\", 'she', \"she's\", 'should', 'shouldn', 'shouldnt', \"shouldn't\", \"should've\", 'si', 'since', 'six', 'so', 'soch', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'still', 'sub', 'such', 'sup', 'sure', 't', 'tab', 'tabh', 'tak', 'take', 'taken', 'tarah', 'teen', 'teeno', 'teesra', 'teesre', 'teesri', 'tell', 'tends', 'tera', 'tere', 'teri', 'th', 'tha', 'than', 'thank', 'thanks', 'thanx', 'that', \"that'll\", 'thats', \"that's\", 'the', 'theek', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'theres', \"there's\", 'thereupon', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'thi', 'thik', 'thing', 'think', 'thinking', 'third', 'this', 'tho', 'thoda', 'thodi', 'thorough', 'thoroughly', 'those', 'though', 'thought', 'three', 'through', 'throughout', 'thru', 'thus', 'tjhe', 'to', 'together', 'toh', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'true', 'truly', 'try', 'trying', 'tu', 'tujhe', 'tum', 'tumhara', 'tumhare', 'tumhari', 'tune', 'twice', 'two', 'um', 'umm', 'un', 'under', 'unhe', 'unhi', 'unho', 'unhone', 'unka', 'unkaa', 'unke', 'unki', 'unko', 'unless', 'unlikely', 'unn', 'unse', 'until', 'unto', 'up', 'upar', 'upon', 'us', 'use', 'used', 'useful', 'uses', 'usi', 'using', 'uska', 'uske', 'usne', 'uss', 'usse', 'ussi', 'usually', 'vaala', 'vaale', 'vaali', 'vahaan', 'vahan', 'vahi', 'vahin', 'vaisa', 'vaise', 'vaisi', 'vala', 'vale', 'vali', 'various', 've', 'very', 'via', 'viz', 'vo', 'waala', 'waale', 'waali', 'wagaira', 'wagairah', 'wagerah', 'waha', 'wahaan', 'wahan', 'wahi', 'wahin', 'waisa', 'waise', 'waisi', 'wala', 'wale', 'wali', 'want', 'wants', 'was', 'wasn', 'wasnt', \"wasn't\", 'way', 'we', \"we'd\", 'well', \"we'll\", 'went', 'were', \"we're\", 'weren', 'werent', \"weren't\", \"we've\", 'what', 'whatever', \"what's\", 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', \"where's\", 'whereupon', 'wherever', 'whether', 'which', 'while', 'who', 'whoever', 'whole', 'whom', \"who's\", 'whose', 'why', 'will', 'willing', 'with', 'within', 'without', 'wo', 'woh', 'wohi', 'won', 'wont', \"won't\", 'would', 'wouldn', 'wouldnt', \"wouldn't\", 'y', 'ya', 'yadi', 'yah', 'yaha', 'yahaan', 'yahan', 'yahi', 'yahin', 'ye', 'yeah', 'yeh', 'yehi', 'yes', 'yet', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\", 'yup']\n"
     ]
    }
   ],
   "source": [
    "print(hind_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a5e0e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38e99bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sun.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa107ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a763217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "881e34db",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword.append(\"Domain specific stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "233f1e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'Domain specific stopwords']\n"
     ]
    }
   ],
   "source": [
    "print(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44884e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos-part of speech-noun, pronoun\n",
    "\n",
    "avoid removing stopword in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be33af3f",
   "metadata": {},
   "source": [
    "### Stemming and Lemmitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5848582",
   "metadata": {},
   "outputs": [],
   "source": [
    "both are techniques used to reduce words to their base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd23534",
   "metadata": {},
   "outputs": [],
   "source": [
    "I walked....>walk ed\n",
    "I am walking...>walk ing\n",
    "I will Walk....> \n",
    "He walks.....> walk s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fd3058",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "directly rule base opproch\n",
    "directly suffix can remove\n",
    "It is the process of reducing word to its root/base\n",
    "from by removing suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a64f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "their are 2 problems-\n",
    "1.overstemming-two or more unrelated words result in \n",
    "twoo same stem\n",
    "\n",
    "2.understemming-two or more related words result in \n",
    "twoo diffenrent stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e354794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#three algorithms\n",
    "\n",
    "porterstemmer- support english language only\n",
    "snoballstememr- support multiple language\n",
    "lancaster stemmer- is a aggressive stemmer(news---new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c368a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agar aapko ifficiancy jada important hai to lemmetization\n",
    "jada use krne wale hai to ham dictionary se look up karte hai\n",
    "jab data bahot jada hai to ye apka lemmalizer kya krega time lega\n",
    "to jaise retrivals me ham jab compairs karenge dono ko\n",
    "waha pe examples leke dekhenge so retrivals me kya hota hai\n",
    "mostly ham stemmers ka use kr skte hai(information retrivel me)\n",
    "jaise ham google mai google kya kr raha hai information retrive krke de raha hai\n",
    "to us case me agar ham stemmer ka use krte hai to jada \n",
    "benifit rahta hai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ced1ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6061c6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_Stemmer =PorterStemmer()\n",
    "snowball_Stemmer=SnowballStemmer(\"english\")\n",
    "lancaster_Stemmer=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2e3fd5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porter_Stemmer ---> rel\n",
      "snowball_Stemmer  --> relat\n",
      "lancaster_Stemmer ---> rel\n"
     ]
    }
   ],
   "source": [
    "word=\"relativity\"\n",
    "print(\"porter_Stemmer --->\", porter_Stemmer.stem(word))\n",
    "print(\"snowball_Stemmer  -->\", snowball_Stemmer.stem(word))\n",
    "print(\"lancaster_Stemmer --->\", lancaster_Stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "73f346af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porter_Stemmer ---> mice\n",
      "snowball_Stemmer  --> mice\n",
      "lancaster_Stemmer ---> mic\n"
     ]
    }
   ],
   "source": [
    "word=\"mice\"\n",
    "print(\"porter_Stemmer --->\", porter_Stemmer.stem(word))\n",
    "print(\"snowball_Stemmer  -->\", snowball_Stemmer.stem(word))\n",
    "print(\"lancaster_Stemmer --->\", lancaster_Stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a219b995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porter_Stemmer ---> maximum\n",
      "snowball_Stemmer  --> maximum\n",
      "lancaster_Stemmer ---> maxim\n"
     ]
    }
   ],
   "source": [
    "word=\"maximum\"\n",
    "print(\"porter_Stemmer --->\", porter_Stemmer.stem(word))\n",
    "print(\"snowball_Stemmer  -->\", snowball_Stemmer.stem(word))\n",
    "print(\"lancaster_Stemmer --->\", lancaster_Stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f42d35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porter_Stemmer ---> privat\n",
      "snowball_Stemmer  --> privat\n",
      "lancaster_Stemmer ---> priv\n"
     ]
    }
   ],
   "source": [
    "word=\"private\"\n",
    "print(\"porter_Stemmer --->\", porter_Stemmer.stem(word))\n",
    "print(\"snowball_Stemmer  -->\", snowball_Stemmer.stem(word))\n",
    "print(\"lancaster_Stemmer --->\", lancaster_Stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "75385f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porter_Stemmer ---> news\n",
      "snowball_Stemmer  --> news\n",
      "lancaster_Stemmer ---> new\n"
     ]
    }
   ],
   "source": [
    "word=\"news\"\n",
    "print(\"porter_Stemmer --->\", porter_Stemmer.stem(word))\n",
    "print(\"snowball_Stemmer  -->\", snowball_Stemmer.stem(word))\n",
    "print(\"lancaster_Stemmer --->\", lancaster_Stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "688ff549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porter_Stemmer ---> telecommun\n",
      "snowball_Stemmer  --> telecommun\n",
      "lancaster_Stemmer ---> telecommun\n"
     ]
    }
   ],
   "source": [
    "word=\"telecommunication\"\n",
    "print(\"porter_Stemmer --->\", porter_Stemmer.stem(word))\n",
    "print(\"snowball_Stemmer  -->\", snowball_Stemmer.stem(word))\n",
    "print(\"lancaster_Stemmer --->\", lancaster_Stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ecaf4439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porter_Stemmer ---> media\n",
      "snowball_Stemmer  --> media\n",
      "lancaster_Stemmer ---> med\n"
     ]
    }
   ],
   "source": [
    "word=\"media\"\n",
    "print(\"porter_Stemmer --->\", porter_Stemmer.stem(word))\n",
    "print(\"snowball_Stemmer  -->\", snowball_Stemmer.stem(word))\n",
    "print(\"lancaster_Stemmer --->\", lancaster_Stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d23b4e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porter_Stemmer ---> wander\n",
      "snowball_Stemmer  --> wander\n",
      "lancaster_Stemmer ---> wand\n"
     ]
    }
   ],
   "source": [
    "word=\"wander\"\n",
    "print(\"porter_Stemmer --->\", porter_Stemmer.stem(word))\n",
    "print(\"snowball_Stemmer  -->\", snowball_Stemmer.stem(word))\n",
    "print(\"lancaster_Stemmer --->\", lancaster_Stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fcbffcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porter_Stemmer ---> previou\n",
      "snowball_Stemmer  --> previous\n",
      "lancaster_Stemmer ---> prevy\n"
     ]
    }
   ],
   "source": [
    "word=\"previous\"\n",
    "print(\"porter_Stemmer --->\", porter_Stemmer.stem(word))\n",
    "print(\"snowball_Stemmer  -->\", snowball_Stemmer.stem(word))\n",
    "print(\"lancaster_Stemmer --->\", lancaster_Stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a52a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "only sufix ke upar work krta h ... meaningful word nhi dekhta \n",
    "h.... every time triming.... uske liye ham lemmitization use krta h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8c2a0e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porter_Stemmer ---> wonder\n",
      "snowball_Stemmer  --> wonder\n",
      "lancaster_Stemmer ---> wond\n"
     ]
    }
   ],
   "source": [
    "word=\"wonderer\"\n",
    "print(\"porter_Stemmer --->\", porter_Stemmer.stem(word))\n",
    "print(\"snowball_Stemmer  -->\", snowball_Stemmer.stem(word))\n",
    "print(\"lancaster_Stemmer --->\", lancaster_Stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "52022d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porter_Stemmer ---> pre-process\n",
      "snowball_Stemmer  --> pre-process\n",
      "lancaster_Stemmer ---> pre-processing\n"
     ]
    }
   ],
   "source": [
    "word=\"pre-processing\"\n",
    "print(\"porter_Stemmer --->\", porter_Stemmer.stem(word))\n",
    "print(\"snowball_Stemmer  -->\", snowball_Stemmer.stem(word))\n",
    "print(\"lancaster_Stemmer --->\", lancaster_Stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3144e202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porter_Stemmer ---> unhappi\n",
      "snowball_Stemmer  --> unhappi\n",
      "lancaster_Stemmer ---> unhappy\n"
     ]
    }
   ],
   "source": [
    "word=\"unhappiness\"\n",
    "print(\"porter_Stemmer --->\", porter_Stemmer.stem(word))\n",
    "print(\"snowball_Stemmer  -->\", snowball_Stemmer.stem(word))\n",
    "print(\"lancaster_Stemmer --->\", lancaster_Stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710315f8",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6386a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "It is a process of reducing words to its lemma.\n",
    "lemmatization condider cotext and intended meaning of wor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401e48e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "using wordnet lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "75dc72ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/admin/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "90527612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "315d2e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ec60a166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happiness'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"happiness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6aead139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"running\") # bydefault takes noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1dc3e46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meaning'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"meaning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "86445296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lovable'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"lovable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b2444a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"running\",\"v\") # verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "79cb15fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'news'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"news\",\"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba6fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stripes\n",
    "stripe -->noun (shirt stripes)\n",
    "strip -->verb (peeling or removing outer layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "75033b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strip\n",
      "stripe\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"stripes\",\"v\"))\n",
    "print(lemmatizer.lemmatize(\"stripes\",\"n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef063f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos hv important role in this----sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6498e74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "is\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv a b c b s g\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sent\u001b[38;5;241m.\u001b[39msplit(), pos\u001b[38;5;241m.\u001b[39msplit()):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#print(i,j)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(i,j))\n",
      "File \u001b[0;32m~/MY/anaconda3/lib/python3.11/site-packages/nltk/stem/wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m wn\u001b[38;5;241m.\u001b[39m_morphy(word, pos)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[0;32m~/MY/anaconda3/lib/python3.11/site-packages/nltk/corpus/reader/wordnet.py:2072\u001b[0m, in \u001b[0;36mWordNetCorpusReader._morphy\u001b[0;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[1;32m   2064\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_morphy\u001b[39m(\u001b[38;5;28mself\u001b[39m, form, pos, check_exceptions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   2065\u001b[0m     \u001b[38;5;66;03m# from jordanbg:\u001b[39;00m\n\u001b[1;32m   2066\u001b[0m     \u001b[38;5;66;03m# Given an original string x\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;66;03m# 3. If there are no matches, keep applying rules until you either\u001b[39;00m\n\u001b[1;32m   2070\u001b[0m     \u001b[38;5;66;03m#    find a match or you can't go any further\u001b[39;00m\n\u001b[0;32m-> 2072\u001b[0m     exceptions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_map[pos]\n\u001b[1;32m   2073\u001b[0m     substitutions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMORPHOLOGICAL_SUBSTITUTIONS[pos]\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_rules\u001b[39m(forms):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'b'"
     ]
    }
   ],
   "source": [
    "sent=\"running is a great way to stay fit.\"\n",
    "pos=\"v a b c b s g\"\n",
    "for i, j in zip(sent.split(), pos.split()):\n",
    "    #print(i,j)\n",
    "    print(lemmatizer.lemmatize(i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cdb634b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i,j\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"i,j\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9bb7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## why need stemming:\n",
    "simplicity:\n",
    "efficiency:\n",
    "    \n",
    "early stages of text analysis:\n",
    "large dataset:stemming preferable    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c042d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b8cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24711d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b817ff00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583164b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "speling\n",
    "andd\n",
    "snd----> and, sand, send"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790e44ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit distence- how many edits to transfor, one string to\n",
    "another\n",
    "---> insertion\n",
    "---> deletion\n",
    "---> substistution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdc8d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip installl TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47269017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2364d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "html tag remove, url, ,puntuation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900d1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "row string=\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5844575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6bdeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"account: 86565779778   \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad30d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "html tag remove, url, ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00f510d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url1=\"this is colab link:https://meet.google.com/iva-bwdm-hqg\"\n",
    "pattern=\"http?:[//]\\s*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e26c6522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'http?:[//]\\s*', re.UNICODE)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ulr_pattern=re.compile(pattern)\n",
    "ulr_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2aba204a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sub() missing 1 required positional argument: 'string'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ult_pattern\u001b[38;5;241m=\u001b[39mre\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,html_pattern)\n",
      "\u001b[0;31mTypeError\u001b[0m: sub() missing 1 required positional argument: 'string'"
     ]
    }
   ],
   "source": [
    "ult_pattern=re.sub(\"\",html_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a0e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44f45fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_pattern=re.compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adcd0e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_pattern=re.sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba37102e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Beautifulsoap' from 'bs4' (/Users/admin/MY/anaconda3/lib/python3.11/site-packages/bs4/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Beautifulsoap\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Beautifulsoap' from 'bs4' (/Users/admin/MY/anaconda3/lib/python3.11/site-packages/bs4/__init__.py)"
     ]
    }
   ],
   "source": [
    "from bs4 import Beautifulsoap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b612e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "abrevations-short form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5192e8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bb50b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3268cf3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (547290558.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[20], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    if word.lower() in abbre_dict,keys():\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "word=\"comple this task\"\n",
    "for word in word.split():\n",
    "    if word.lower() in abbre_dict,keys():\n",
    "        word=abbre_dict[word.lower]\n",
    "        word_list.append(word)\n",
    "    else: \n",
    "        word_list.append(word)\n",
    "        \n",
    "\" \".join(word_list)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd660565",
   "metadata": {},
   "source": [
    "text representatiom\n",
    "\n",
    "1. One-Hot Encoding\n",
    "Each word is represented as a vector with a length equal to the size of the vocabulary. All values in the vector are 0 except for a single 1 at the index corresponding to the word.\n",
    "\n",
    "Example\n",
    "For a vocabulary [\"cat\", \"dog\", \"mouse\"]:\n",
    "\n",
    "\"cat\" → [1, 0, 0]\n",
    "\"dog\" → [0, 1, 0]\n",
    "\"mouse\" → [0, 0, 1]\n",
    "\n",
    "2. Bag of Words (BoW)\n",
    "Represents text as a vector of word counts or binary values indicating the presence of words. It disregards word order and grammar but captures word frequency.\n",
    "\n",
    "Example\n",
    "For the sentences:\n",
    "\n",
    "\"I love dogs\"\n",
    "\"I love cats and dogs\"\n",
    "Vocabulary: [\"I\", \"love\", \"dogs\", \"cats\", \"and\"]\n",
    "\n",
    "\"I love dogs\" → [1, 1, 1, 0, 0]\n",
    "\"I love cats and dogs\" → [1, 1, 1, 1, 1\n",
    "\n",
    "3. Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "A more sophisticated version of the BoW that considers the importance of a word in a document relative to a collection of documents (corpus). It scales down the frequency of common words and scales up the frequency of rare words.\n",
    "\n",
    "Example\n",
    "For the same sentences, the TF-IDF vectors might differ in scale depending on the corpus.\n",
    "\n",
    "4. Word Embeddings\n",
    "Dense vector representations of words learned through neural networks, capturing semantic meanings and relationships. Common word embedding techniques include Word2Vec, GloVe, and FastText.\n",
    "\n",
    "Example\n",
    "Using Word2Vec, \"king\" and \"queen\" might have similar vectors, reflecting their related meanings.\n",
    "\n",
    "5. Sentence Embeddings\n",
    "Represent entire sentences or phrases as vectors, capturing the meaning and context. Techniques include Doc2Vec, Universal Sentence Encoder, and BERT embeddings.\n",
    "\n",
    "Example\n",
    "Using Universal Sentence Encoder, the sentences \"I love dogs\" and \"I adore canines\" would have similar embeddings.\n",
    "\n",
    "6. Contextual Embeddings\n",
    "Advanced techniques like BERT, GPT, and ELMo generate embeddings that capture the context of words within sentences, providing different embeddings for the same word depending on its context.\n",
    "\n",
    "Example\n",
    "The word \"bank\" in:\n",
    "\n",
    "\"I went to the bank to deposit money.\"\n",
    "\"The river bank was eroding.\"\n",
    "would have different embeddings in BERT, reflecting its different meanings.\n",
    "\n",
    "Example Usage in Python\n",
    "Here's how you might use some of these techniques in Python using popular libraries:\n",
    "\n",
    "One-Hot Encoding\n",
    "python\n",
    "Copy code\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "vocab = [[\"cat\"], [\"dog\"], [\"mouse\"]]\n",
    "encoder.fit(vocab)\n",
    "encoded = encoder.transform([[\"cat\"]])\n",
    "print(encoded)  # Output: [[1. 0. 0.]]\n",
    "Bag of Words\n",
    "python\n",
    "Copy code\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\"I love dogs\", \"I love cats and dogs\"]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())\n",
    "Word Embeddings (Using Gensim's Word2Vec)\n",
    "python\n",
    "Copy code\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [[\"I\", \"love\", \"dogs\"], [\"I\", \"love\", \"cats\", \"and\", \"dogs\"]]\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "vector = model.wv[\"dogs\"]\n",
    "print(vector)\n",
    "Contextual Embeddings (Using Transformers)\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"I love dogs\"\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "print(last_hidden_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c528110",
   "metadata": {},
   "source": [
    "Problems with One-Hot Encoding\n",
    "High Dimensionality: Each word is represented by a long, sparse vector, which is inefficient and hard to manage for large vocabularies.\n",
    "\n",
    "No Semantic Information: It doesn't capture the meaning or relationships between words (e.g., \"cat\" and \"dog\" are completely unrelated).\n",
    "Fixed Vocabulary: New or out-of-vocabulary words can't be handled without changing the entire encoding scheme.\n",
    "\n",
    "Better Alternatives\n",
    "Word Embeddings: Methods like Word2Vec or GloVe create dense vectors that capture semantic relationships between words.\n",
    "Contextual Embeddings: Models like BERT provide vectors that take the word's context into account, giving more meaningful representations.\n",
    "Example\n",
    "One-Hot Encoding: \"cat\" → [1, 0, 0, 0] (no info about relationships or meaning)\n",
    "Word Embedding: \"cat\" → [0.25, -0.1, 0.33, 0.4] (captures meaning and relationships)\n",
    "Word embeddings and contextual embeddings are more efficient and meaningful for NLP tasks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab480c0",
   "metadata": {},
   "source": [
    "N-gram Range\n",
    "Why: N-grams are contiguous sequences of n words from a given text. They capture local word order and context better than single words (unigrams). Using n-grams can improve the performance of models, especially in tasks like text classification and language modeling.\n",
    "\n",
    "Unigrams (1-gram): Single words (\"the\", \"dog\", \"is\")\n",
    "Bigrams (2-gram): Pairs of consecutive words (\"the dog\", \"dog is\")\n",
    "Trigrams (3-gram): Triplets of consecutive words (\"the dog is\")\n",
    "Example:\n",
    "\n",
    "Original: \"The dog is in the park.\"\n",
    "Bigrams: [\"the dog\", \"dog is\", \"is in\", \"in the\", \"the park\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
